{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predictor.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNMerBoWEMPJG5CXEBaZB/N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fzanart/GHDomains/blob/main/Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBq391WRUSES"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U sentence-transformers\n",
        "!pip install flaml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from flaml import AutoML\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from sklearn.metrics import classification_report, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import markdown\n",
        "import lxml\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "import pickle"
      ],
      "metadata": {
        "id": "kTVnitItUq0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports:\n",
        "\n",
        "# Clasification model\n",
        "with open('/content/drive/MyDrive/GHDomains/Resources/automl.pkl', 'rb') as fp:\n",
        "  automl = pickle.load(fp)\n",
        "# Label encoder\n",
        "with open('/content/drive/MyDrive/GHDomains/Resources/automl.pkl', 'rb') as fp:\n",
        "  le = pickle.load(fp)\n",
        "# Num scaler\n",
        "with open('/content/drive/MyDrive/GHDomains/Resources/automl.pkl', 'rb') as fp:\n",
        "  scaler = pickle.load(fp)\n",
        "# Column names\n",
        "with open('/content/drive/MyDrive/GHDomains/Resources/automl.pkl', 'rb') as fp:\n",
        "  column_names = pickle.load(fp)"
      ],
      "metadata": {
        "id": "RuIK0kY0W4cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
      ],
      "metadata": {
        "id": "q3YKoz7eeXF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def join_words_from_lists(text, join=False):\n",
        "  text = str(text)\n",
        "  if text == 'nan':\n",
        "    return ''\n",
        "  else:\n",
        "    text = text[1:-1]\n",
        "    text = text.replace('\\'','')\n",
        "    text = text.split(',')\n",
        "    if join:\n",
        "      text = ' '.join([word.strip() for word in text])\n",
        "    return text\n",
        "\n",
        "def clean_htmltags(text):\n",
        "    html = markdown.markdown(text)                                # convert the code to html\n",
        "    soup = BeautifulSoup(html, \"lxml\")                            # html.parser #XML stands for \"eXtensible Markup Language\",\n",
        "    \n",
        "    [s.extract() for s in soup(['iframe', 'script'])]             # eliminate html tags \n",
        "    stripped_text = soup.get_text()                               # get the rest of the text\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', ' ', stripped_text)  # delete remaining tags\n",
        "    text = stripped_text                                          # voila!\n",
        "                                    \n",
        "    return text\n",
        "\n",
        "def vectorize_categorical(df_Serie, prefix):\n",
        "    lp_index = df_Serie.index\n",
        "\n",
        "    vect = CountVectorizer(tokenizer=join_words_from_lists)\n",
        "    lp_Serie = vect.fit_transform(df_Serie.astype(str))\n",
        "\n",
        "    lp_Serie = pd.DataFrame(lp_Serie.toarray(), columns=vect.get_feature_names_out(), index=lp_index).add_prefix(prefix)\n",
        "    lp_Serie = lp_Serie.astype(pd.SparseDtype(np.int64))\n",
        "    return lp_Serie"
      ],
      "metadata": {
        "id": "BTVrmMFocFeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_transform(df, selected_features):\n",
        "\n",
        "  # Apply text pre-processing\n",
        "  df['Labels'] = df['Labels'].apply(lambda x: join_words_from_lists(x, join=True))\n",
        "  df['Readme'] = df['Readme'].astype(str)\n",
        "  df['Readme'] = pd.Series([clean_htmltags(x) for x in lp['Readme']]) #df['Readme'].apply(lambda x: clean_htmltags(x)) #\n",
        "  df['Readme'] = df['Readme'].apply(lambda x: re.sub('\\s{2,}', \" \", x))\n",
        "  df['Description'] = df['Description'].astype(str)\n",
        "  df['Description'] = df['Description'].replace('nan','')\n",
        "  df['Text_data']   = df['Description'] + df['Readme'] + df['Labels']\n",
        "  transformed_df = model.encode(df['Text_data'].to_list())\n",
        "\n",
        "  # Vectorise categorical features\n",
        "  cn_vect = vectorize_categorical(df['Contributors'], 'cn: ')\n",
        "  pl_vect = vectorize_categorical(df['Languages'], 'pl: ')\n",
        "  tp_vect = vectorize_categorical(df['Topics'], 'tp: ')\n",
        "  ct_vect = vectorize_categorical(df['Contents'], 'ct: ')\n",
        "  li_vect = vectorize_categorical(df['Licence'], 'li: ')\n",
        "\n",
        "  # transform numbers of stars, forks y releases\n",
        "  num_vectors = df[['Stars','Forks', 'Releases']].astype(np.float64)\n",
        "  num_columns = num_vectors.columns\n",
        "  num_index = num_vectors.index\n",
        "  num_vectors = scaler.transform(num_vectors)\n",
        "  num_vectors = pd.DataFrame(num_vectors, columns=num_columns, index=num_index)\n",
        "\n",
        "  # Get same columns to plug into the model\n",
        "  transformed_df = pd.DataFrame(transformed_df, index=df.index).add_prefix('Text ')\n",
        "  transformed_df = pd.concat([transformed_df, cn_vect, pl_vect, tp_vect, ct_vect, li_vect, num_vectors], axis=1)\n",
        "  transformed_df = df.reindex(columns=selected_features, fill_value=0)\n",
        "\n",
        "  return transformed_df"
      ],
      "metadata": {
        "id": "WbfrVSQMeD3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = automl.predict(fit_transform(df, column_names).to_numpy())"
      ],
      "metadata": {
        "id": "K3pmakjDh8hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: fix clasification results. for target names use le.clases_\n",
        "# Report for less popular, anotated dataset.\n",
        "print(classification_report(lp_true, lp_pred, target_names=le.classes_))"
      ],
      "metadata": {
        "id": "22OfmPRNhmos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For new popular, save the results.\n",
        "# TODO: Read the datasets."
      ],
      "metadata": {
        "id": "Z3jORxJjhpui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wbcO3SWxh2e3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}